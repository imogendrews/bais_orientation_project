<script>
  import { P, Heading } from "flowbite-svelte";
</script>

<main>
  <div class="p-10">
    <Heading class="text-black pb-2" tag="h2">Result presentation</Heading>

    <Heading class="text-black pb-2" tag="h4">Introduction</Heading>
    <P class="text-black pb-2">
      For my orientation project, The Default, I decided to explore the default output of images
      generated by one of the latest versions of Stable Diffusion. Inspired by the Implicit
      Association Test (IAT), developed by psychologists Anthony Greenwald, Mahzarin Banaji, and
      Brian Nosek in 1998 to reveal human biases, I wanted to create a project that examines the
      biases present in this image generation model. Since the emergence of AI-generated imagery,
      companies like Stable Diffusion have faced criticism for the biases embedded in their models.
      With each new version, they strive to address these issues and make improvements. My goal was
      to investigate which biases persist in one of the latest versions of Stable Diffusion, so I
      conducted this analysis using version 3.5.
    </P>

    <Heading class="text-black pb-2" tag="h3">Technical Implementation and context</Heading>

    <Heading class="text-black pb-2" tag="h4">Image Generation</Heading>
    <P class="text-black pb-2"
      >As outlined in my project plan, I first reviewed the research paper Smiling Women Pitching
      Down: Auditing Representational and Presentational Gender Biases (Sun, Wei, Sun, Suh, Shen, &
      Yang, 2024) from the University of Wisconsin-Madison to understand previous studies on this
      topic. After reading the study, I decided to structure this section of my project around 20
      prompts, generating 100 images for each one.</P
    >
    <P class="text-black pb-2"
      >I began by downloading Stable Diffusion 3.5 from Hugging Face onto one of the desktops in the
      CTech room. Using ComfyUI, I generated 100 images for each of five different prompts. My goal
      is to eventually generate 100 images for all 20 prompts; however, I underestimated how long
      the process would take. Below is an example of my ComfyUI setup:</P
    >
    <img />

    <P class="text-black pb-2"
      >To speed up the process, I ran these prompts overnight over several days at the university. I
      set the batch size to 10 and queued the prompt 10 times, with each batch taking around six
      hours to complete. Initially, I attempted to use a batch size of 100, but the program failed
      to run due to limited GPU memory. I also opted not to reduce the resolution of the images, as
      anything below 1024x1024 produced overly simplistic results that did not fully reflect the
      capabilities of the latest version of Stable Diffusion.</P
    >
    <Heading class="text-black pb-2" tag="h3">Juypter and Python</Heading>
    <P class="text-black pb-2">
      Next, I wrote a Python script and ran it in Jupyter, using a machine learning captioning model
      called BLIP to generate captions for each image. I created a separate GitHub repository for
      this, which you can view here. In the script word2vec_analysis.py, you can read about each
      step of my process. In summary, I first generated captions for each image using BLIP2. This is
      the latest BLIP model, released in 2023, which I obtained from Hugging Face. The first version
      of this model was released in 2022. BLIP stands for Bootstrapping Language-Image Pre-training,
      and it was developed by Salesforce AI Research.
    </P>
    <P class="text-black pb-2"
      >I then trained the word2vec model, which creates a vector representation for each word,
      called a word embedding, and finds the semantic relationships between words. I wanted to use
      this so that, from the captions generated, I could compile a list of words that frequently
      appeared within each prompt to see if certain objects were mentioned more often than others.</P
    >
    <P class="text-black pb-2"
      >I then defined a list of gender-related words and youth-related words. For the gender-related
      words, I attached either ‘male’, ‘female’, or ‘neutral’ to each image so I could see which
      genders were most prevalent in each prompt. For age, I attached an age identifier to each
      image that mentioned age, or indicated its absence.</P
    >
    <P class="text-black pb-2"
      >ed to display all my findings in a table using Pandas, including the image, image file name,
      gender, and age words associated with each image. Below that, I displayed the results of my
      word2vec analysis in the Word Frequency Analysis section. Here, I also included statistics for
      the age-related and gender-related words. You can view my results for each prompt in the files
      ending with ‘.ipynb’.</P
    >
    <Heading class="text-black pb-2" tag="h3">Data Visualisation</Heading>
    <Heading class="text-black pb-2" tag="h4">App Structure</Heading>
    <P class="text-black pb-2"
      >For the visualisation, I planned the overall structure of my app with the help of the reading
      Narrative Visualization: Telling Stories with Data by Edward Segel and Jeffrey Heer. This
      paper explores various data visualization techniques, examining what each one aims to achieve
      and how to implement it. I found this particularly helpful since I am completely new to data
      visualization and had no idea where to begin.</P
    >
    <P class="text-black pb-2"
      >The authors introduced the concept of author-driven narrative and reader-driven narrative,
      with different visualizations relying more on one or the other, depending on the narrative
      being told. Author-driven narratives are more linear and include less interactivity, while
      reader-driven narratives are non-linear and involve more interactivity.</P
    >
    <P class="text-black pb-2"
      >I ultimately decided to use the Martini Glass Structure, which incorporates both
      author-driven and reader-driven narratives at different stages of the user experience. This
      structure resembles a martini glass, “with the stem representing the single-path,
      author-driven narrative and the widening mouth of the glass representing the available paths
      made possible through reader-driven interactivity.” For my project, I plan to use a slideshow
      when the user first opens the website, employing an author-driven narrative to explain why and
      how I am investigating Stable Diffusion 3.5. At the end of the slideshow, the website
      transitions to a reader-driven narrative, allowing users to freely interact with the data
      presented.</P
    >
    <Heading class="text-black pb-2" tag="h4">App Design</Heading>
    <P class="text-black pb-2"
      >For the design aspect of this app, I was inspired by a data visualization called Is the Love
      Song Dying by David Mora and Michelle Jia. I really appreciated how they conveyed their point
      through narration. Unfortunately, I wasn’t able to fully develop my introduction as richly as
      I had hoped, but I was still inspired by their approach. I think the main takeaway from their
      project was the idea of making captions appear from each dot. In their project, you can hover
      over dots on a page, with each one representing a different love song. These dots change
      dynamically based on what they are visually showing, which I thought was a simple yet powerful
      way to present the data.</P
    >

    <Heading class="text-black pb-2" tag="h3">Discussion of Your Results and Future Work</Heading>
    <P class="text-black pb-2"
      >You can see my results on both the core website and in the above-mentioned GitHub project.
      The results on the website only show which gender is most predominant in each prompt. However,
      I was able to generate much more detailed statistics, which you can see in my GitHub project.
      As mentioned above, I generated statistics on the most prevalent objects using a Word2Vec
      model, as well as all gender-related words and age-related words. In the future, I would like
      to display all these findings in a more engaging way and improve the visualization by
      incorporating a variety of techniques.</P
    >
    <P
      >Overall, the results show that the latest versions of both Stable Diffusion and BLIP2 still
      have a long way to go in terms of representing society in a more inclusive manner. This is
      evident in several ways, but the most striking issue is gender representation. A clear example
      of this is how the prompt "A doctor" generates images mostly of men, whereas "A nurse"
      generates predominantly images of women. Men are also predominantly represented in most
      prompts, with the exception of "A beautiful person" and "A teacher."</P
    >
    <P
      >In the future, I would also like to use a different caption generation model. BLIP2 is
      useful, but it does not provide as much depth as I would like. Additionally, it would be great
      to find a model that I can tweak to consistently provide specific types of analysis, such as
      gender. My current version falls short in this regard because, although it usually mentions
      gender in the caption, it does not do so consistently enough, which could lead to
      misrepresentation of the data. Furthermore, I was only able to collect data on gender and age.
      I had initially intended to examine race and weight discrimination as well, but the captions
      were too inconsistent in describing these attributes.</P
    >
    <P
      >Regarding race, I found that the model exhibited bias by only occasionally mentioning if
      someone was Black or Asian, while never specifying if someone was White. Additionally, the
      generated images never actually depicted different body types, except in response to the
      prompt "A prisoner." This is concerning, but I was unable to gather enough statistics to
      create a clear visualization. Perhaps, with more time, I can refine my approach and explore
      these aspects further.</P
    >
    <Heading class="text-black pb-2" tag="h2">Project documentation</Heading>
    <Heading class="text-black pb-2" tag="h3">Categories</Heading>
    <P
      >I believe the main categories my project covered were narrative development,
      research/experimentation, and software development. In my project plan, I mentioned
      creative/artistic development. However, since I was only able to reach the stage of a minimal
      viable product, I wasn’t able to fully explore all the creative possibilities within data
      visualization—something I would like to do in the future.</P
    >
    <P
      >In terms of narrative development, this is reflected in how I structured the project
      according to the Martini Glass Structure. My research and experimentation are evident in the
      various papers I read to acquire the necessary skills for this project. Lastly, my software
      development efforts can be seen in the app I created and the process I used to generate and
      analyze the images.</P
    >
    <Heading class="text-black pb-2" tag="h3">Technical Choices</Heading>
    <P
      >For the machine learning models I used, I decided to explore Stable Diffusion primarily
      because I had learned about diffusion models in my Machine Learning 2 workshop and wanted to
      explore them further. I then chose BLIP for captioning after reading that it was one of the
      most widely used models for this task. Since my goal was to examine the current "default"
      representation of each prompt, I wanted to use a model that was popular at the time of the
      project’s creation. Lastly, I used a Word2Vec model to analyze all the objects found in the
      captions, as I had previously worked with it in a project at my job and found it to be
      effective.</P
    >
    <P
      >For data analysis, I chose Pandas after hearing that it was one of the best frameworks
      available. Since I had not worked with Python before, I wanted to use an established library
      with extensive documentation. Initially, I planned to store my data in a database, but I
      decided against it since the JSON files were not too large, and I wanted to avoid potential
      loading time issues. Instead, I uploaded my images to Cloudinary, as they were the largest
      data files and could not be stored locally in a free GitHub project.</P
    >
    <P
      >For data visualization, I chose D3.js because I am new to the field and wanted to use a
      widely adopted tool. Lastly, I built my application in JavaScript using the Svelte framework,
      as I had used it in my previous job as a frontend developer and wanted to work with a
      framework I was already familiar with.</P
    >
    <Heading class="text-black pb-2" tag="h3">What worked well, what didn't?</Heading>
    <P
      >I think the caption generation for each image, the analysis I was able to derive from it, and
      its visualization using D3.js worked really well. However, I did not anticipate how long it
      would take to generate the images, and a significant amount of my time was spent on this early
      in the process. As a result, I fell behind on my timeline. This primarily impacted the
      visualization and UI/UX aspects of my website, which were not as polished as I had hoped.
      Unfortunately, I wasn’t able to implement all the features I had planned, which is a pity.</P
    >
  </div>
</main>
